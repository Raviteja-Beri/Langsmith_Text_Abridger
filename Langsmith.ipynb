{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbb9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ENTER_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ENTER_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please resposne to the user request only based on the given context\"),\n",
    "        (\"user\",\"Question:{question}\\nContext:{context}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech discusses the recent advancements in Large Language Models (LLMs), highlighting their remarkable capabilities and the influx of research contributions in the field. It covers various topics such as architectural innovations, training strategies, context length improvements, multi-modal LLMs, robotics, datasets, benchmarking, and efficiency. The article aims to provide a comprehensive overview and reference for researchers and practitioners to draw insights from existing works and advance LLM research.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "question=\"Can you summarize the speech?\"\n",
    "\n",
    "context='''Abstract\n",
    "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
    "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
    "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
    "robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
    "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
    "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
    "yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\n",
    "on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\n",
    "concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\n",
    "provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\n",
    "extensive informative summaries of the existing works to advance the LLM research.\n",
    "Keywords:\n",
    "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
    "1. Introduction\n",
    "Language plays a fundamental role in facilitating communication and self-expression for humans, and their interaction\n",
    "with machines. The need for generalized models stems from\n",
    "the growing demand for machines to handle complex language\n",
    "tasks, including translation, summarization, information retrieval, conversational interactions, etc. Recently, significant\n",
    "breakthroughs have been witnessed in language models, primarily attributed to transformers [1], increased computational\n",
    "capabilities, and the availability of large-scale training data.\n",
    "These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks'''\n",
    "\n",
    "print(chain.invoke({\"question\":question,\"context\":context}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
